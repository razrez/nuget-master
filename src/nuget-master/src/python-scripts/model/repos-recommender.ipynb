{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Только при необходимости в Google Colab или локально, если нет нужных пакетов:\n",
    "%pip install sentence-transformers scikit-learn pandas numpy\n",
    "# Если планируется fine-tuning: \n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Загрузка большого датасета репозиториев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big repos head:\n",
      "                          Repository  \\\n",
      "0                  leiurayer/downkyi   \n",
      "1  BluePointLilac/ContextMenuManager   \n",
      "2            microsoft/reverse-proxy   \n",
      "3         Kyome22/RunCat_for_windows   \n",
      "4                         dotnet/tye   \n",
      "\n",
      "                                         Description  \n",
      "0                                       downkyi8khdr  \n",
      "1                                            windows  \n",
      "2  toolkit developing highperformance http revers...  \n",
      "3         cute running cat animation windows taskbar  \n",
      "4  tye tool makes developing testing deploying mi...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Предположим, что у нас есть CSV-файл \"big_repos.csv\",\n",
    "# который содержит столбцы: repoName, description\n",
    "big_repos_df = pd.read_csv(r\"C:\\Users\\sharp\\Desktop\\nuget-master\\src\\nuget-master\\src\\python-scripts\\dataset\\preprocessed_txtdata.csv\")\n",
    "# Убедимся, что колонки действительно существуют:\n",
    "print(\"Big repos head:\")\n",
    "print(big_repos_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Загрузка небольшого ручного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset head:\n",
      "                          query  \\\n",
      "0                      jwt auth   \n",
      "1                      jwt auth   \n",
      "2       Kafka messages consumer   \n",
      "3       Kafka messages consumer   \n",
      "4  Weather forecast application   \n",
      "\n",
      "                                            repoName  \\\n",
      "0   mostakahammed/JwtAuthenticationWithEFCoreIdenity   \n",
      "1                           Andrej-Gorlov/ProductAPI   \n",
      "2                confluentinc/confluent-kafka-dotnet   \n",
      "3                           ExactTargetDev/kafka-net   \n",
      "4  Nucleus-center-of-excellence/weatherman-dotnet...   \n",
      "\n",
      "                                    repo_description  label  \n",
      "0  Implement JWT Authentication & Role base Autho...      1  \n",
      "1                                        Product API      1  \n",
      "2               Confluent's Apache Kafka .NET client      1  \n",
      "3  This is a .NET implementation of a client for ...      1  \n",
      "4  The idea of the application is to design a Wea...      1  \n"
     ]
    }
   ],
   "source": [
    "# Это набор [query, repoName, repo_description, label].\n",
    "# label = 1 (релевантно), 0 (нерелевантно).\n",
    "# Если нет repo_description в самом файле, \n",
    "#   можно подтягивать из big_repos_df. Но пусть будет сразу:\n",
    "\n",
    "labeled_df = pd.read_csv(r\"C:\\Users\\sharp\\Desktop\\nuget-master\\src\\nuget-master\\src\\python-scripts\\dataset\\test_dataset_with_descriptions.csv\")\n",
    "print(\"Labeled dataset head:\")\n",
    "print(labeled_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Подключение предобученной модели Sentence-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8160e93f0fec420ca4886cc62a37d393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2dad8892459408face0ddc70bc6752c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b30dbf94f104702b677f4d44ae03e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd294eb1348344d99b5633b9981a8fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7e1e3bb2e949cfa305cea864447ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5429fc0e353e4595afcd426509cd7b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d680706eba864d6d80d2cdac8d440e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e33a72196b4125b6f9329f2a3f0914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f6de5b4ae541ec8bd605e7826641ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9be660d2bda496c9fd2eea4d60600e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a18ad2b61f483dbacee3f850067f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Загрузка конкретной модели\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# Или 'multi-qa-MiniLM-L6-cos-v1', 'all-mpnet-base-v2', 'all-distilroberta-v1' и т.д.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Генерация эмбеддингов для основного набора репозиториев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# 4.1 Извлекаем описания\n",
    "all_repo_texts = big_repos_df['Description'].fillna(\"\").tolist()\n",
    "\n",
    "# 4.2 Векторизуем (batch-encode)\n",
    "print(\"Encoding main repo descriptions...\")\n",
    "repo_embeddings = model.encode(all_repo_texts, \n",
    "                               batch_size=32, \n",
    "                               show_progress_bar=True, \n",
    "                               convert_to_numpy=True)\n",
    "\n",
    "# 4.3 Строим kNN индекс (по косинусному сходству)\n",
    "knn = NearestNeighbors(metric='cosine', n_neighbors=5)\n",
    "knn.fit(repo_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовим быстрый доступ к repoName: \n",
    "# (чтобы при поиске по индексу знать, какой репозиторий вернули)\n",
    "repo_names = big_repos_df['Repository'].tolist()  # список по индексам 0..N-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Функция «поиска» для произвольного запроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'jwt auth' -> top 5 repos:\n",
      " [{'repoIndx': 4570, 'repoName': 'cornflourblue/dotnet-6-jwt-authentication-api'}, {'repoIndx': 1612, 'repoName': 'cornflourblue/dotnet-6-jwt-authentication-api'}, {'repoIndx': 670, 'repoName': 'cornflourblue/dotnet-6-jwt-authentication-api'}, {'repoIndx': 6007, 'repoName': 'cornflourblue/dotnet-6-jwt-authentication-api'}, {'repoIndx': 3053, 'repoName': 'cornflourblue/dotnet-6-jwt-authentication-api'}]\n"
     ]
    }
   ],
   "source": [
    "def search_top_k(query_text, k=5):\n",
    "    \"\"\"\n",
    "    Возвращает список словарей вида \n",
    "    [\n",
    "      {repoIndx: i, repoName: \"...\"},\n",
    "      {repoIndx: j, repoName: \"...\"},\n",
    "      ...\n",
    "    ]\n",
    "    наиболее релевантных к запросу (по косинусной близости).\n",
    "    \"\"\"\n",
    "    query_emb = model.encode([query_text], convert_to_numpy=True)\n",
    "    distances, indices = knn.kneighbors(query_emb, n_neighbors=k)\n",
    "\n",
    "    # indices[0] - это индексы в big_repos_df\n",
    "    results = []\n",
    "    for rank, idx in enumerate(indices[0]):\n",
    "        repo_info = {\n",
    "            'repoIndx': idx,\n",
    "            'repoName': repo_names[idx]\n",
    "        }\n",
    "        results.append(repo_info)\n",
    "    return results\n",
    "\n",
    "# Тест\n",
    "query = \"jwt auth\"\n",
    "results = search_top_k(query, k=5)\n",
    "print(f\"Query: '{query}' -> top 5 repos:\\n\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Работа с ручным датасетом (train/val/test) для теста и/или fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 Разбиваем labeled_df\n",
    "Если у нас 30–40 примеров, делаем:\n",
    "\n",
    "* 20% (test) — в сторону, для финальной проверки\n",
    "* 80% (train+val) — для обучения и/или подбора параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Val: 33, Test: 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if len(labeled_df) > 0:\n",
    "    trainval_df, test_df = train_test_split(labeled_df, test_size=0.2, random_state=42)\n",
    "    print(f\"Train+Val: {len(trainval_df)}, Test: {len(test_df)}\")\n",
    "else:\n",
    "    trainval_df, test_df = None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Если мы не дообучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (no fine-tuning) P@5=0.025, R@5=0.111\n"
     ]
    }
   ],
   "source": [
    "def compute_precision_recall_at_k(df_test, k=5):\n",
    "    \"\"\"\n",
    "    df_test: DataFrame c колонками [query, repoName, label].\n",
    "    Для каждой строки с label=1 проверяем, попадает ли repoName в top-k результатов search_top_k(query).\n",
    "    Возвращаем общий precision, recall по всем query.\n",
    "    \"\"\"\n",
    "    total_predictions = 0         # sum(K) по всем запросам\n",
    "    total_relevant = 0           # сколько всего label=1\n",
    "    total_correct = 0            # скольких релевантных мы верно поймали\n",
    "\n",
    "    queries = df_test['query'].unique()\n",
    "    for q in queries:\n",
    "        subset = df_test[df_test['query'] == q]\n",
    "        # Сколько релевантных (label=1) для этого q\n",
    "        relevant_subset = subset[subset['label'] == 1]\n",
    "        num_relevant = len(relevant_subset)\n",
    "        total_relevant += num_relevant\n",
    "\n",
    "        # Выполним поиск top-k\n",
    "        top_results = search_top_k(q, k=k)\n",
    "        top_repo_names = {r['repoName'] for r in top_results}\n",
    "\n",
    "        # Подсчитываем, сколько из релевантных попало в top-k\n",
    "        correct_for_q = 0\n",
    "        for idx, row in relevant_subset.iterrows():\n",
    "            if row['repoName'] in top_repo_names:\n",
    "                correct_for_q += 1\n",
    "\n",
    "        total_correct += correct_for_q\n",
    "        total_predictions += k  # для всех запросов выдаём k репозиториев\n",
    "\n",
    "    precision = total_correct / total_predictions if total_predictions > 0 else 0.0\n",
    "    recall = total_correct / total_relevant if total_relevant > 0 else 0.0\n",
    "    return precision, recall\n",
    "\n",
    "# Если есть test_df, считаем\n",
    "if test_df is not None and len(test_df) > 0:\n",
    "    p5, r5 = compute_precision_recall_at_k(test_df, k=5)\n",
    "    print(f\"Baseline (no fine-tuning) precision@5={p5:.3f}, recall@5={r5:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохранимс модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_model.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('model/trained_models/sbert_model_folder')\n",
    "joblib.dump(knn, 'knn_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3 (Опционально Если хотим дообучить) Fine-tuning (Siamese/Bi-Encoder) на trainval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Итоговое использование\n",
    "* model (Sentence-BERT, возможно не дообученная),\n",
    "* repo_embeddings — векторное представление каждого репозитория из big_repos_df,\n",
    "* knn — построенный индекс для быстрого поиска top-K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_top_k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m input_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m top_results \u001b[38;5;241m=\u001b[39m search_top_k(input_query, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rep \u001b[38;5;129;01min\u001b[39;00m top_results: \u001b[38;5;28mprint\u001b[39m(rep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepoName\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'search_top_k' is not defined"
     ]
    }
   ],
   "source": [
    "input_query = \"kafka adapter\"\n",
    "top_results = search_top_k(input_query, k=5)\n",
    "\n",
    "for rep in top_results: print(rep['repoName'])\n",
    "# print(top_results)\n",
    "# [{repoIndx:..., repoName:...}, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyGithub\n",
    "import sys\n",
    "import re\n",
    "from github import Github, GithubException \n",
    "from github import Auth\n",
    "\n",
    "\n",
    "def get_packages_from(repository: str):\n",
    "\n",
    "    # Get the repository object\n",
    "    repo = g.get_repo(repository)\n",
    "\n",
    "    # Get all branches in the repository\n",
    "    branches = repo.get_branches()\n",
    "\n",
    "    # Get the default branch (usually the root branch)\n",
    "    default_branch = next((branch for branch in branches if branch.name == repo.default_branch), None)\n",
    "\n",
    "    # Get the root tree of the repository\n",
    "    tree = repo.get_git_tree(sha=default_branch.commit.sha, recursive=True)\n",
    "\n",
    "    # Filter out only the.csproj files\n",
    "    csproj_files = []\n",
    "    for item in tree.tree:\n",
    "        if item.type == \"blob\" and item.path.endswith(\".csproj\"):\n",
    "            csproj_files.append(item)\n",
    "\n",
    "    # Extract PackageReference from each.csproj file\n",
    "    package_references = set()\n",
    "    for file in csproj_files:\n",
    "        file_content = repo.get_contents(file.path).decoded_content.decode(\"utf-8\")\n",
    "        pattern = r'PackageReference Include=\"([^\"]+)\"'\n",
    "        matches = re.findall(pattern, file_content)\n",
    "        package_references.update(matches)\n",
    "\n",
    "    return package_references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
