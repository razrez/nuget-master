#!/usr/bin/env python
# -*- coding: utf-8 -*-

# import sys
# import csv
# import requests
# import json
# import time

# # %pip install re
# # %pip install nltk
# # %pip install unicodedata
# # %pip install contractions
# # %pip install inflect
# # %pip install emoji

# import re
# import os
# import nltk
# import emoji
# import unicodedata
# import contractions
# import inflect
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize

# # –¥–ª—è —ç—Ç–æ–≥–æ —Ç–æ–∂ pip install –Ω—É–∂–Ω–æ –ø—Ä–æ–ø–∏—Å–∞—Ç—å
# # nltk.download('stopwords')
# # nltk.download('punkt')
# # nltk.download('averaged_perceptron_tagger')

# # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —á–∞—Å—Ç–µ—Ä–µ—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞ –ø—É—Ç—ë–º —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Spacy, –∑–∞–≥—Ä—É–∑–∫–∏ perceptron_tagger –∏ –º–æ–¥—É–ª—è Spacy en
# # %pip install spacy
# #!python -m spacy download en_core_web_sm
# import spacy

# # Commented out IPython magic to ensure Python compatibility.
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import sys
# import warnings
# warnings.filterwarnings('ignore')
# import re
# import csv
# from sklearn.cluster import KMeans

# import sys
# np.set_printoptions(suppress=True)
# np.set_printoptions(threshold=sys.maxsize)
# np.set_printoptions(precision=3)

# import warnings
# warnings.filterwarnings('ignore')
# import joblib

# # %pip install nltk
# # %pip install unicodedata
# # %pip install contractions
# # %pip install inflect
# # %pip install emoji


# # from sklearn.metrics.pairwise import cosine_similarity
# # import gensim
# # pip install scipy==1.12 !!!!!!!!!!!!!
# # pip install scikit-learn==1.2.2 !!!!!
# from gensim.models import FastText
# from sklearn.neighbors import NearestNeighbors

# """–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥"""

# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
# def clean_text(input_text):

#     # HTML-—Ç–µ–≥–∏: –ø–µ—Ä–≤—ã–π —à–∞–≥ - —É–¥–∞–ª–∏—Ç—å –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ HTML-—Ç–µ–≥–∏
#     clean_text = re.sub('<[^<]+?>', '', input_text)

#     # URL –∏ —Å—Å—ã–ª–∫–∏: –¥–∞–ª–µ–µ - —É–¥–∞–ª—è–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ URL –∏ —Å—Å—ã–ª–∫–∏
#     clean_text = re.sub(r'http\S+', '', clean_text)

#     # –≠–º–æ–¥–∂–∏ –∏ —ç–º–æ—Ç–∏–∫–æ–Ω—ã: —É–¥–∞–ª—è–µ–º –∏—Ö –Ω–∞—Ñ–∏–≥, —ç—Ç–æ –∫—Ä–∏–Ω–∂ –∫–∞–∫–æ–π-—Ç–æ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π, —Ç–æ–ª—å–∫–æ —à—É–º–∞ –¥–æ–±–∞–≤—è—Ç
#     clean_text = remove_emojis(clean_text)

#     # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
#     clean_text = clean_text.lower()

#     # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã
#     # –¢–∞–∫ –∫–∞–∫ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Ç–µ–ø–µ—Ä—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–ª–æ–≤–∞–º–∏ - —É–¥–∞–ª–∏–º –ø—Ä–æ–±–µ–ª—ã
#     clean_text = re.sub(r'\s+', ' ', clean_text)

#     # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤ —Å –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –∑–Ω–∞–∫–∞–º–∏ –∫ ASCII-—Å–∏–º–≤–æ–ª–∞–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é normalize –∏–∑ –º–æ–¥—É–ª—è unicodedata –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–∏–º–≤–æ–ª—ã —Å –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –∑–Ω–∞–∫–∞–º–∏ –∫ ASCII-—Å–∏–º–≤–æ–ª–∞–º
#     clean_text = unicodedata.normalize('NFKD', clean_text).encode('ascii', 'ignore').decode('utf-8', 'ignore')

#     # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è: —Ç–µ–∫—Å—Ç —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤—Ä–æ–¥–µ "don't" –∏–ª–∏ "won't", –ø–æ—ç—Ç–æ–º—É —Ä–∞–∑–≤–µ—Ä–Ω—ë–º –ø–æ–¥–æ–±–Ω—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
#     clean_text = contractions.fix(clean_text)

#     # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
#     clean_text = re.sub(r'c\#', 'csharp', clean_text)
#     clean_text = re.sub(r'c\+\+', 'cpp', clean_text)

#     # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: –∏–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç –≤—Å–µ–≥–æ, —á—Ç–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è "—Å–ª–æ–≤–∞–º–∏"
#     clean_text = re.sub(r'[^a-zA-Z0-9\s]', '', clean_text)

#     # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —á–∏—Å–ª–∞ –ø—Ä–æ–ø–∏—Å—å—é: 100 –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ "—Å—Ç–æ" (–¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–∞)
#     temp = inflect.engine()
#     words = []
#     for word in clean_text.split():
#         if word.isdigit():
#             words.append(temp.number_to_words(word))
#         else:
#             words.append(word)
#     clean_text = ' '.join(words)

#     # –°—Ç–æ–ø-—Å–ª–æ–≤–∞: —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ - —ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤
#     stop_words = set(stopwords.words('english'))
#     tokens = word_tokenize(clean_text)
#     tokens = [token for token in tokens if token not in stop_words]
#     clean_text = ' '.join(tokens)

#     # Add full-stop to end of sentences
#     clean_text = re.sub(r'([a-z])\.([A-Z])', r'\1. \2', clean_text)

#     # –ó–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è: –¥–∞–ª–µ–µ - —É–¥–∞–ª—è–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
#     clean_text = re.sub(r'[^\w\s]', '', clean_text)

#     # –ò –Ω–∞–∫–æ–Ω–µ—Ü - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
#     return clean_text


# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç–º–æ–¥–∂–∏ –≤ —Å–ª–æ–≤–∞
# def emojis_to_txt(text):

#     # –ú–æ–¥—É–ª—å emoji: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∂–∏ –≤ –∏—Ö —Å–ª–æ–≤–µ—Å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
#     clean_text = emoji.demojize(text, delimiters=(" ", " "))

#     # –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø—É—Ç—ë–º –∑–∞–º–µ–Ω—ã ":" –∏" _", –∞ —Ç–∞–∫ –∂–µ - –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–µ–ª–∞ –º–µ–∂–¥—É –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
#     clean_text = clean_text.replace(":", "").replace("_", " ")

#     return clean_text


# def remove_emojis(text):
#     emoji_pattern = re.compile(
#         "["
#         "\U00010000-\U0010FFFF"  # Unicode characters beyond the Basic Multilingual Plane
#         "\u200d"  # Zero-width joiner
#         "\u2640-\u2642"  # Male and female signs
#         "\u2600-\u26FF"  # Miscellaneous symbols
#         "\u2700-\u27BF"  # Dingbats
#         "]+",
#         flags=re.UNICODE
#     )
#     return emoji_pattern.sub(r'', text)


# # %pip install spacy
# # nltk.download('averaged_perceptron_tagger')
# #!python -m spacy download en_core_web_sm

# nlp = spacy.load('en_core_web_sm')
# lemmatize_exceptions = ['ios', 'mmorts']

# def lemmatize_and_postag(input_text):

#     doc = nlp(input_text)
#     lemmatize_output = []

#     # Iterate over each token in the document
#     for token in doc:
#         if token.text in lemmatize_exceptions:
#             lemmatize_output.append(token.text)
#             # lemmatize_output.append(token.text + '_' + token.pos_)
#         else:
#             # Append the token lemma and its POS tag to the tagged_output list
#             lemmatize_output.append(token.lemma_)

#     # Join the tagged_output list into a single string
#     lemmatize_output_str = ' '.join(lemmatize_output)

#     return lemmatize_output_str


# def preprocesse(input_text):
#     clear_text = clean_text(input_text)

#     tagged_output = lemmatize_and_postag(clear_text)

#     return tagged_output


# # Preprocess the text data
# def preprocess_query(query_text):
#     query_text = preprocesse(query_text)
#     return query_text.split()

# # get the vector representation of a query
# def get_query_vector(query, model):
#     words = preprocess_query(query)
#     query_vector = model.wv.get_sentence_vector(words)
#     return query_vector

# # get the vector representation of a repository
# def get_repository_vector(description, model):
#     words = description
#     repo_vector = model.wv.get_sentence_vector(words)
#     return repo_vector

# # function to retrieve relevant repositories for a given query
# def retrieve_relevant_repositories(query, knn, model, data):
#     query_vector = get_query_vector(query, model)
#     distances, indices = knn.kneighbors([query_vector])
#     return data.Repository.iloc[indices[0]].values.tolist()


# current_dir = os.path.dirname(__file__)

# """FastText + kNN(kd-tree)"""
# """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∏ –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ"""
# # data = pd.read_csv(r'C:\Users\sharp\Desktop\nuget-master\src\nuget-master\src\python-scripts\dataset\preprocessed_txtdata.csv')
# data = pd.read_csv(os.path.join(current_dir, 'dataset', 'preprocessed_txtdata.csv'))
# data['Description'] = data['Description'].apply(lambda x: [word.split('_')[0] for word in x.split()])

# # fasttext_model = FastText.load(r"C:\Users\sharp\Desktop\nuget-master\src\nuget-master\src\python-scripts\model\trained_models\fasttext_model")
# # knn = joblib.load(r'C:\Users\sharp\Desktop\nuget-master\src\nuget-master\src\python-scripts\model\trained_models\knn_model.joblib')
# fasttext_model = FastText.load(os.path.join(current_dir, 'model', 'trained_models', 'fasttext_model'))
# knn = joblib.load(os.path.join(current_dir, 'model', 'trained_models', 'knn_model.joblib'))

# while True:
#     try:
#         line = sys.stdin.readline().strip()
#         ##line = sys.argv[1]
#         if line:
#             query = line
#             relevant_repositories = retrieve_relevant_repositories(query, knn, fasttext_model, data)
#             print(",".join(relevant_repositories))

#     except Exception as e: print("error")

###### new version
## pip install sentence-transformers scikit-learn pandas numpy
import os
import sys
import re
import unicodedata
import joblib
import spacy
import emoji
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors

###############################################################################
# 0. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
###############################################################################
def emojis_to_txt(text):
    """
    –ú–æ–¥—É–ª—å emoji: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∂–∏ –≤ –∏—Ö —Å–ª–æ–≤–µ—Å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
    –ù–∞–ø—Ä–∏–º–µ—Ä, üòä -> :smiling_face_with_smiling_eyes:
    """
    clean_text = emoji.demojize(text, delimiters=(" ", " "))
    # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–µ—Ç–æ—á–∏—è –∏ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã —Ç–µ–∫—Å—Ç –±—ã–ª –±–æ–ª–µ–µ —á–∏—Ç–∞–µ–º
    clean_text = clean_text.replace(":", "").replace("_", " ")
    return clean_text

def remove_emojis(text):
    """
    –£–¥–∞–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤ –Æ–Ω–∏–∫–æ–¥–∞, –æ—Ç–Ω–æ—Å—è—â–∏—Ö—Å—è –∫ —ç–º–æ–¥–∂–∏, —Å–∏–º–≤–æ–ª–∞–º –∏ —Ç.–¥.
    """
    emoji_pattern = re.compile(
        "["
        "\U00010000-\U0010FFFF"  # Unicode beyond BMP
        "\u200d"                 # Zero-width joiner
        "\u2640-\u2642"          # Male/female signs
        "\u2600-\u26FF"          # Misc. symbols
        "\u2700-\u27BF"          # Dingbats
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.sub(r'', text)

###############################################################################
# 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è spaCy –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (–ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏)
###############################################################################
nlp = spacy.load('en_core_web_sm')
# –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç—å ¬´–Ω–µ–Ω—É–∂–Ω—ã–µ¬ª —Å–ª–æ–≤–∞, –¥–æ–±–∞–≤—å—Ç–µ –∏—Ö –≤ exceptions
lemmatize_exceptions = ['ios', 'mmorts']

def lemmatize_text(input_text):
    """
    –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º (–ø—Ä–∏–≤–æ–¥–∏–º —Å–ª–æ–≤–∞ –∫ —Å–ª–æ–≤–∞—Ä–Ω–æ–π —Ñ–æ—Ä–º–µ)
    """
    doc = nlp(input_text)
    output_tokens = []
    for token in doc:
        if token.text in lemmatize_exceptions:
            output_tokens.append(token.text)
        else:
            output_tokens.append(token.lemma_)
    return " ".join(output_tokens)

###############################################################################
# 2. –§—É–Ω–∫—Ü–∏—è ¬´–æ–±—â–µ–π¬ª –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
###############################################################################
def clean_text(input_text):
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º —ç–º–æ–¥–∂–∏ –≤ ¬´:smile:¬ª
    demojized = emojis_to_txt(input_text)
    # –ü–æ—Ç–æ–º —É–¥–∞–ª—è–µ–º ¬´–Ω–∞—Å—Ç–æ—è—â–∏–µ¬ª —ç–º–æ–¥–∂–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –æ—Å—Ç–∞—Ç—å—Å—è
    no_emoji = remove_emojis(demojized)
    # Normalize unicode (NFKD), —É–±—Ä–∞—Ç—å –¥–∏–∞–∫—Ä–∏—Ç–∏–∫—É, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ
    # no_emoji = unicodedata.normalize('NFKD', no_emoji).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    lemmatized = lemmatize_text(no_emoji)
    return lemmatized.lower().strip()

###############################################################################
# 3. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (—Å –ø–æ–º–æ—â—å—é clean_text)
###############################################################################
def preprocess_query(query_text):
    """
    –í—ã–ø–æ–ª–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É (—ç–º–æ–¥–∂–∏, –ª–µ–º–º–∞...) + 
    –∑–¥–µ—Å—å –º–æ–∂–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –Ω–æ Sentence-BERT —Å–∞–º —É–º–µ–µ—Ç —Ä–∞–∑–±–∏–≤–∞—Ç—å —Ç–µ–∫—Å—Ç
    """
    return clean_text(query_text)

###############################################################################
# 4. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ kNN –∏–Ω–¥–µ–∫—Å–∞
###############################################################################
# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —Å–∫—Ä–∏–ø—Ç –ª–µ–∂–∏—Ç —Ä—è–¥–æ–º —Å –ø–∞–ø–∫–æ–π model/trained_models
current_dir = os.path.dirname(__file__)

# - –ú–æ–¥–µ–ª—å Sentence-BERT (—Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è)
#   –í—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –ø–∞–ø–∫—É sbert_model_folder, –≥–¥–µ –ª–µ–∂–∞—Ç config.json, etc.
model_path = os.path.join(current_dir, 'model', 'trained_models', 'sbert_model_folder')
model = SentenceTransformer(model_path)

# - kNN –∏–Ω–¥–µ–∫—Å (NearestNeighbors), —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π joblib-–æ–º
knn_path = os.path.join(current_dir, 'model', 'trained_models', 'knn_model.joblib')
knn = joblib.load(knn_path)

# 5. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ (CSV)
data_path = os.path.join(current_dir, 'dataset', 'preprocessed_txtdata.csv')
data = pd.read_csv(data_path)



# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –≤ data –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∏:
#   Repository  (–Ω–∞–∑–≤–∞–Ω–∏–µ)
#   Description (—Ç–µ–∫—Å—Ç, –ª–∏–±–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
# –ï—Å–ª–∏ Description —É –≤–∞—Å –º–∞—Å—Å–∏–≤ —Ç–æ–∫–µ–Ω–æ–≤, –∞ S-BERT –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å—Ç—Ä–æ–∫—É, 
# –º–æ–∂–µ—Ç–µ —Å–∫–ª–µ–∏—Ç—å –∏—Ö: " ".join(Description)

###############################################################################
# 6. –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤
###############################################################################
def retrieve_relevant_repositories(query, knn, model, data):
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
    clean_q = preprocess_query(query)
    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
    query_vector = model.encode([clean_q])  # shape: (1, dim)

    # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏—Ö
    distances, indices = knn.kneighbors(query_vector, n_neighbors=5)
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π (–∏–ª–∏ full rows)
    repos = data.Repository.iloc[indices[0]].values.tolist()
    return repos

###############################################################################
# 7. –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —á—Ç–µ–Ω–∏—è query –∏–∑ stdin, –≤—ã–≤–æ–¥–∏–º top-5
###############################################################################
if __name__ == "__main__":

    while True:
        try:
            line = sys.stdin.readline().strip()
            if not line:
                break  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ -> –≤—ã—Ö–æ–¥–∏–º

            query = line
            relevant = retrieve_relevant_repositories(query, knn, model, data)
            # –í—ã–≤–æ–¥–∏–º —Å–ø–∏—Å–∫–æ–º —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
            print(",".join(relevant))

        except Exception as e:
            print(f"error: {e}")
            break
